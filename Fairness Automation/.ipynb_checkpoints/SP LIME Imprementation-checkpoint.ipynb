{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c53059d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, KBinsDiscretizer\n",
    "from aif360.sklearn.datasets import fetch_adult\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed285c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WeightedDataset(X=                   age     workclass     education  education-num  \\\n",
       "race      sex                                                       \n",
       "Non-white Male    25.0       Private          11th            7.0   \n",
       "White     Male    38.0       Private       HS-grad            9.0   \n",
       "          Male    28.0     Local-gov    Assoc-acdm           12.0   \n",
       "Non-white Male    44.0       Private  Some-college           10.0   \n",
       "White     Male    34.0       Private          10th            6.0   \n",
       "...                ...           ...           ...            ...   \n",
       "          Female  27.0       Private    Assoc-acdm           12.0   \n",
       "          Male    40.0       Private       HS-grad            9.0   \n",
       "          Female  58.0       Private       HS-grad            9.0   \n",
       "          Male    22.0       Private       HS-grad            9.0   \n",
       "          Female  52.0  Self-emp-inc       HS-grad            9.0   \n",
       "\n",
       "                      marital-status         occupation   relationship   race  \\\n",
       "race      sex                                                                   \n",
       "Non-white Male         Never-married  Machine-op-inspct      Own-child  Black   \n",
       "White     Male    Married-civ-spouse    Farming-fishing        Husband  White   \n",
       "          Male    Married-civ-spouse    Protective-serv        Husband  White   \n",
       "Non-white Male    Married-civ-spouse  Machine-op-inspct        Husband  Black   \n",
       "White     Male         Never-married      Other-service  Not-in-family  White   \n",
       "...                              ...                ...            ...    ...   \n",
       "          Female  Married-civ-spouse       Tech-support           Wife  White   \n",
       "          Male    Married-civ-spouse  Machine-op-inspct        Husband  White   \n",
       "          Female             Widowed       Adm-clerical      Unmarried  White   \n",
       "          Male         Never-married       Adm-clerical      Own-child  White   \n",
       "          Female  Married-civ-spouse    Exec-managerial           Wife  White   \n",
       "\n",
       "                     sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "race      sex                                                          \n",
       "Non-white Male      Male           0.0           0.0            40.0   \n",
       "White     Male      Male           0.0           0.0            50.0   \n",
       "          Male      Male           0.0           0.0            40.0   \n",
       "Non-white Male      Male        7688.0           0.0            40.0   \n",
       "White     Male      Male           0.0           0.0            30.0   \n",
       "...                  ...           ...           ...             ...   \n",
       "          Female  Female           0.0           0.0            38.0   \n",
       "          Male      Male           0.0           0.0            40.0   \n",
       "          Female  Female           0.0           0.0            40.0   \n",
       "          Male      Male           0.0           0.0            20.0   \n",
       "          Female  Female       15024.0           0.0            40.0   \n",
       "\n",
       "                 native-country  \n",
       "race      sex                    \n",
       "Non-white Male    United-States  \n",
       "White     Male    United-States  \n",
       "          Male    United-States  \n",
       "Non-white Male    United-States  \n",
       "White     Male    United-States  \n",
       "...                         ...  \n",
       "          Female  United-States  \n",
       "          Male    United-States  \n",
       "          Female  United-States  \n",
       "          Male    United-States  \n",
       "          Female  United-States  \n",
       "\n",
       "[45222 rows x 13 columns], y=race       sex   \n",
       "Non-white  Male      <=50K\n",
       "White      Male      <=50K\n",
       "           Male       >50K\n",
       "Non-white  Male       >50K\n",
       "White      Male      <=50K\n",
       "                     ...  \n",
       "           Female    <=50K\n",
       "           Male       >50K\n",
       "           Female    <=50K\n",
       "           Male      <=50K\n",
       "           Female     >50K\n",
       "Name: annual-income, Length: 45222, dtype: category\n",
       "Categories (2, object): ['<=50K' < '>50K'], sample_weight=race       sex   \n",
       "Non-white  Male      226802.0\n",
       "White      Male       89814.0\n",
       "           Male      336951.0\n",
       "Non-white  Male      160323.0\n",
       "White      Male      198693.0\n",
       "                       ...   \n",
       "           Female    257302.0\n",
       "           Male      154374.0\n",
       "           Female    151910.0\n",
       "           Male      201490.0\n",
       "           Female    287927.0\n",
       "Name: fnlwgt, Length: 45222, dtype: float64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = fetch_adult()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbcb4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adult():\n",
    "    data_df = pd.read_csv(f\"adult.csv\")\n",
    "    # Drop the target column\n",
    "    TARGET_COLUMNS = data_df.columns[-1]\n",
    "    data = data_df.drop(columns=[TARGET_COLUMNS])\n",
    "\n",
    "    data, numeric_columns, categorical_columns = preprocess_dataset(data, continuous_features=[])\n",
    "    data_df_copy = data.copy()\n",
    "    # Scale the dataset\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    data_scaled = min_max_scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(data_scaled, columns=data.columns)\n",
    "\n",
    "    FEATURE_COLUMNS = data.columns\n",
    "\n",
    "    # Add the target column back\n",
    "    data[TARGET_COLUMNS] = data_df[TARGET_COLUMNS]\n",
    "\n",
    "    return data, FEATURE_COLUMNS, TARGET_COLUMNS, numeric_columns, categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2fb5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df, continuous_features=[]):\n",
    "    label_encoder = LabelEncoder()\n",
    "    onehot_encoder = OneHotEncoder()\n",
    "\n",
    "    numeric_columns = []\n",
    "    categorical_columns = []\n",
    "\n",
    "    # Iterate over each column in the DataFrame\n",
    "    for col in df.columns:\n",
    "        # Check if the column is categorical\n",
    "        if df[col].dtype == 'object' or df[col].dtype == 'category' and col not in continuous_features:\n",
    "            categorical_columns.append(col)\n",
    "            # If the column has only two unique values, treat it as binary categorical\n",
    "            if len(df[col].unique()) == 2:\n",
    "                # Label encode binary categorical features\n",
    "                df[col] = label_encoder.fit_transform(df[col])\n",
    "            else:\n",
    "                # One-hot encode regular categorical features\n",
    "                encoded_values = onehot_encoder.fit_transform(df[[col]])\n",
    "                # Create new column names for the one-hot encoded features\n",
    "                new_cols = [col + '_' + str(i) for i in range(encoded_values.shape[1])]\n",
    "                # Convert the encoded values to a DataFrame and assign column names\n",
    "                encoded_df = pd.DataFrame(encoded_values.toarray(), columns=new_cols)\n",
    "                # Concatenate the encoded DataFrame with the original DataFrame\n",
    "                df = pd.concat([df, encoded_df], axis=1)\n",
    "                # Drop the original categorical column from the DataFrame\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "        # If the column is numerical but in string format and not in continuous_features, convert it to numerical type\n",
    "        elif df[col].dtype == 'object' or df[col].dtype == 'category' and df[\n",
    "            col].str.isnumeric().all() and col not in continuous_features:\n",
    "            df[col] = df[col].astype(int)  # Convert to integer type\n",
    "            categorical_columns.append(col)\n",
    "        # If the column is a continuous feature, discretize it into bins\n",
    "        elif col in continuous_features:\n",
    "            numeric_columns.append(col)\n",
    "            # Calculate the number of bins\n",
    "            num_unique_values = len(df[col].unique())\n",
    "            value_range = df[col].max() - df[col].min()\n",
    "            num_bins = calculate_num_bins(num_unique_values, value_range)\n",
    "\n",
    "            # Discretize into bins\n",
    "            bin_discretizer = KBinsDiscretizer(n_bins=num_bins, encode='ordinal', strategy='uniform')\n",
    "            bins = bin_discretizer.fit_transform(df[[col]])\n",
    "            # Replace the original continuous feature with the binned values\n",
    "            df[col] = bins.astype(int)\n",
    "        else:\n",
    "            # Here are numerical columns. If the column has only 2 unique values, dont add it to numeric_columns\n",
    "            if len(df[col].unique()) > 2:\n",
    "                numeric_columns.append(col)\n",
    "    return df, numeric_columns, categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee0289a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(            age  workclass  education  education-num  marital-status  \\\n",
       " 0      0.301370      0.875   0.600000       0.800000        0.666667   \n",
       " 1      0.452055      0.750   0.600000       0.800000        0.333333   \n",
       " 2      0.287671      0.500   0.733333       0.533333        0.000000   \n",
       " 3      0.493151      0.500   0.066667       0.400000        0.333333   \n",
       " 4      0.150685      0.500   0.600000       0.800000        0.333333   \n",
       " ...         ...        ...        ...            ...             ...   \n",
       " 48837  0.301370      0.500   0.600000       0.800000        0.000000   \n",
       " 48838  0.643836      0.000   0.733333       0.533333        1.000000   \n",
       " 48839  0.287671      0.500   0.600000       0.800000        0.333333   \n",
       " 48840  0.369863      0.500   0.600000       0.800000        0.000000   \n",
       " 48841  0.246575      0.625   0.600000       0.800000        0.333333   \n",
       " \n",
       "        occupation  relationship  race  sex  capital-gain  capital-loss  \\\n",
       " 0        0.071429           0.2  1.00  1.0      0.021740           0.0   \n",
       " 1        0.285714           0.0  1.00  1.0      0.000000           0.0   \n",
       " 2        0.428571           0.2  1.00  1.0      0.000000           0.0   \n",
       " 3        0.428571           0.0  0.50  1.0      0.000000           0.0   \n",
       " 4        0.714286           1.0  0.50  0.0      0.000000           0.0   \n",
       " ...           ...           ...   ...  ...           ...           ...   \n",
       " 48837    0.714286           0.2  1.00  0.0      0.000000           0.0   \n",
       " 48838    0.000000           0.4  0.50  1.0      0.000000           0.0   \n",
       " 48839    0.714286           0.0  1.00  1.0      0.000000           0.0   \n",
       " 48840    0.071429           0.6  0.25  1.0      0.054551           0.0   \n",
       " 48841    0.285714           0.0  1.00  1.0      0.000000           0.0   \n",
       " \n",
       "        hours-per-week  target  \n",
       " 0            0.397959       1  \n",
       " 1            0.122449       1  \n",
       " 2            0.397959       1  \n",
       " 3            0.397959       1  \n",
       " 4            0.397959       1  \n",
       " ...               ...     ...  \n",
       " 48837        0.357143       1  \n",
       " 48838        0.397959       1  \n",
       " 48839        0.500000       1  \n",
       " 48840        0.397959       1  \n",
       " 48841        0.602041       0  \n",
       " \n",
       " [48842 rows x 13 columns],\n",
       " Index(['age', 'workclass', 'education', 'education-num', 'marital-status',\n",
       "        'occupation', 'relationship', 'race', 'sex', 'capital-gain',\n",
       "        'capital-loss', 'hours-per-week'],\n",
       "       dtype='object'),\n",
       " 'target',\n",
       " ['age',\n",
       "  'workclass',\n",
       "  'education',\n",
       "  'education-num',\n",
       "  'marital-status',\n",
       "  'occupation',\n",
       "  'relationship',\n",
       "  'race',\n",
       "  'capital-gain',\n",
       "  'capital-loss',\n",
       "  'hours-per-week'],\n",
       " [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_adult()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eda567e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'workclass', 'education', 'education-num', 'marital-status',\n",
      "       'occupation', 'relationship', 'race', 'sex', 'capital-gain',\n",
      "       'capital-loss', 'hours-per-week', 'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data, FEATURE_COLUMNS, TARGET_COLUMNS, numeric_columns, categorical_columns = load_adult()\n",
    "\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3fb52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def calculate_metrics_and_split_data(data, features, target, classifier_type='logistic_regression',\n",
    "                                     metrics=['accuracy'], attribute=None):\n",
    "    \"\"\"\n",
    "    Train a classifier, predict on test data, calculate specified metrics from the confusion matrix,\n",
    "    and optionally split the test data based on an attribute.\n",
    "\n",
    "    :param data: DataFrame containing the dataset\n",
    "    :param features: List of feature column names\n",
    "    :param target: Name of the target column\n",
    "    :param classifier_type: Type of classifier ('logistic_regression', 'svm', 'random_forest', 'naive_bayes')\n",
    "    :param metrics: List of metrics to calculate ('accuracy', 'precision', 'recall', 'f1', 'tp', 'tn', 'fp', 'fn')\n",
    "    :param attribute: Optional attribute to split the test data on, which should be binary (0 or 1)\n",
    "    :return: Dictionary of requested metrics and optionally two DataFrames, one for each value of the attribute,\n",
    "             and the trained model instance, and the preprocessor\n",
    "    \"\"\"\n",
    "    # Encode categorical features and scale numerical features\n",
    "    categorical_features = data[features].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_features = data[features].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(), categorical_features)])\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Debugging: Print the shape of the train and test sets\n",
    "    print(f\"Training data shape: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"Testing data shape: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "    # Initialize classifier\n",
    "    classifiers = {\n",
    "        'logistic_regression': LogisticRegression(max_iter=500, solver='saga'),\n",
    "        'svm': SVC(probability=True),\n",
    "        'random_forest': RandomForestClassifier(),\n",
    "        'naive_bayes': GaussianNB()\n",
    "    }\n",
    "    classifier = classifiers[classifier_type]\n",
    "\n",
    "    # Fit the classifier and predict\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', classifier)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tp = cm[1, 1]  # True Positives\n",
    "    tn = cm[0, 0]  # True Negatives\n",
    "    fp = cm[0, 1]  # False Positives\n",
    "    fn = cm[1, 0]  # False Negatives\n",
    "\n",
    "    # Debugging: Print the confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics_dict = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, average='binary'),\n",
    "        'recall': recall_score(y_test, y_pred, average='binary'),\n",
    "        'f1': f1_score(y_test, y_pred, average='binary'),\n",
    "        'tp': tp,\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn\n",
    "    }\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f\"Accuracy: {metrics_dict['accuracy']}\")\n",
    "\n",
    "    # Validate metrics\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        if metric not in metrics_dict:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported metric '{metric}'. Choose from 'accuracy', 'precision', 'recall', 'f1', 'tp', 'tn', 'fp', 'fn'.\")\n",
    "        results[metric] = metrics_dict[metric]\n",
    "\n",
    "    # Add target and predictions to test features\n",
    "    X_test_df = X_test.copy()\n",
    "    X_test_df[target] = y_test.values\n",
    "    X_test_df['pred'] = y_pred\n",
    "\n",
    "    # Debugging: Print the test data with all columns\n",
    "    print(\"Test data with all columns:\")\n",
    "    print(X_test_df.head())\n",
    "\n",
    "    return results, X_test_df, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35fe78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tn_fn_dataframes(test_df, target, attribute):\n",
    "    group_0 = test_df[test_df[attribute] == 0]\n",
    "    group_1 = test_df[test_df[attribute] == 1]\n",
    "    print(f\"Group 0 size: {group_0.shape[0]}\")\n",
    "    print(f\"Group 1 size: {group_1.shape[0]}\")\n",
    "    y_true_0 = group_0[target]\n",
    "    y_pred_0 = group_0['pred']\n",
    "    tn_0, fp_0, fn_0, tp_0 = confusion_matrix(y_true_0, y_pred_0).ravel()\n",
    "    print(f\"Confusion Matrix for Group 0: TN={tn_0}, FP={fp_0}, FN={fn_0}, TP={tp_0}\")\n",
    "    y_true_1 = group_1[target]\n",
    "    y_pred_1 = group_1['pred']\n",
    "    tn_1, fp_1, fn_1, tp_1 = confusion_matrix(y_true_1, y_pred_1).ravel()\n",
    "    print(f\"Confusion Matrix for Group 1: TN={tn_1}, FP={fp_1}, FN={fn_1}, TP={tp_1}\")\n",
    " # Create DataFrames for TN and FN\n",
    "    tn_df_0 = group_0[(group_0[target] == 0) & (group_0['pred'] == 0)].drop(columns=[target, 'pred'])\n",
    "    fn_df_0 = group_0[(group_0[target] == 1) & (group_0['pred'] == 0)].drop(columns=[target, 'pred'])\n",
    "\n",
    "    tn_df_1 = group_1[(group_1[target] == 0) & (group_1['pred'] == 0)].drop(columns=[target, 'pred'])\n",
    "    fn_df_1 = group_1[(group_1[target] == 1) & (group_1['pred'] == 0)].drop(columns=[target, 'pred'])\n",
    "\n",
    "    print(f\"TN Group 0 size: {tn_df_0.shape[0]}\")\n",
    "    print(f\"FN Group 0 size: {fn_df_0.shape[0]}\")\n",
    "    print(f\"TN Group 1 size: {tn_df_1.shape[0]}\")\n",
    "    print(f\"FN Group 1 size: {fn_df_1.shape[0]}\")\n",
    "    return tn_df_0, fn_df_0, tn_df_1, fn_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7783a5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: X_train: (39073, 12), y_train: (39073,)\n",
      "Testing data shape: X_test: (9769, 12), y_test: (9769,)\n",
      "Confusion Matrix:\n",
      "[[1048 1307]\n",
      " [ 402 7012]]\n",
      "Accuracy: 0.8250588596581021\n",
      "Test data with all columns:\n",
      "            age  workclass  education  education-num  marital-status  \\\n",
      "7762   0.013699       0.50   0.733333       0.533333        0.666667   \n",
      "23881  0.000000       0.50   0.133333       0.466667        0.666667   \n",
      "30507  0.109589       0.25   0.733333       0.533333        0.666667   \n",
      "28911  0.041096       0.50   1.000000       0.600000        0.666667   \n",
      "19484  0.410959       0.50   0.733333       0.533333        0.666667   \n",
      "\n",
      "       occupation  relationship  race  sex  capital-gain  capital-loss  \\\n",
      "7762     0.571429           0.2   1.0  1.0           0.0           0.0   \n",
      "23881    0.857143           0.6   1.0  0.0           0.0           0.0   \n",
      "30507    0.428571           0.4   0.5  1.0           0.0           0.0   \n",
      "28911    0.857143           0.6   1.0  0.0           0.0           0.0   \n",
      "19484    0.500000           0.8   0.0  1.0           0.0           0.0   \n",
      "\n",
      "       hours-per-week  target  pred  \n",
      "7762         0.193878       1     1  \n",
      "23881        0.193878       1     1  \n",
      "30507        0.397959       1     1  \n",
      "28911        0.295918       1     1  \n",
      "19484        0.561224       1     1  \n",
      "Group 0 size: 3233\n",
      "Group 1 size: 6536\n",
      "Confusion Matrix for Group 0: TN=91, FP=274, FN=20, TP=2848\n",
      "Confusion Matrix for Group 1: TN=957, FP=1033, FN=382, TP=4164\n",
      "TN Group 0 size: 91\n",
      "FN Group 0 size: 20\n",
      "TN Group 1 size: 957\n",
      "FN Group 1 size: 382\n",
      "Results:\n",
      "accuracy: 0.8250588596581021\n",
      "True Positives Group 0 (first 5 rows):\n",
      "            age  workclass  education  education-num  marital-status  \\\n",
      "45197  0.410959      0.250   0.600000       0.800000        0.333333   \n",
      "42010  0.575342      0.750   0.733333       0.533333        1.000000   \n",
      "44004  0.397260      0.625   0.533333       0.666667        0.000000   \n",
      "35859  0.534247      0.500   0.733333       0.533333        0.333333   \n",
      "4215   0.493151      0.000   0.800000       0.866667        0.666667   \n",
      "\n",
      "       occupation  relationship  race  sex  capital-gain  capital-loss  \\\n",
      "45197    0.071429           1.0   1.0  0.0      0.076881           0.0   \n",
      "42010    0.285714           0.2   1.0  0.0      0.135501           0.0   \n",
      "44004    0.285714           0.2   1.0  0.0      0.086141           0.0   \n",
      "35859    0.071429           1.0   1.0  0.0      0.150242           0.0   \n",
      "4215     0.000000           0.2   1.0  0.0      0.086141           0.0   \n",
      "\n",
      "       hours-per-week  \n",
      "45197        0.397959  \n",
      "42010        0.602041  \n",
      "44004        0.704082  \n",
      "35859        0.234694  \n",
      "4215         0.346939  \n",
      "False Positives Group 0 (first 5 rows):\n",
      "            age  workclass  education  education-num  marital-status  \\\n",
      "40834  0.356164        0.5   0.466667       0.733333        0.000000   \n",
      "46571  0.616438        0.5   0.600000       0.800000        0.333333   \n",
      "17016  0.342466        0.5   0.800000       0.866667        0.666667   \n",
      "11665  0.164384        0.5   0.933333       0.933333        0.000000   \n",
      "8780   0.301370        0.5   0.666667       1.000000        0.000000   \n",
      "\n",
      "       occupation  relationship  race  sex  capital-gain  capital-loss  \\\n",
      "40834    0.714286           0.2   1.0  0.0       0.04650           0.0   \n",
      "46571    0.285714           1.0   1.0  0.0       0.04064           0.0   \n",
      "17016    0.285714           0.2   1.0  0.0       0.04650           0.0   \n",
      "11665    0.714286           0.6   1.0  0.0       0.02174           0.0   \n",
      "8780     0.714286           0.8   1.0  0.0       0.00000           0.0   \n",
      "\n",
      "       hours-per-week  \n",
      "40834        0.397959  \n",
      "46571        0.346939  \n",
      "17016        0.397959  \n",
      "11665        0.724490  \n",
      "8780         0.806122  \n",
      "True Positives Group 1 (first 5 rows):\n",
      "            age  workclass  education  education-num  marital-status  \\\n",
      "1276   0.397260      0.500   0.600000       0.800000        0.333333   \n",
      "35505  0.315068      0.125   1.000000       0.600000        0.333333   \n",
      "46820  0.260274      0.500   0.800000       0.866667        0.333333   \n",
      "21063  0.643836      0.875   0.600000       0.800000        0.333333   \n",
      "31256  0.479452      0.750   0.733333       0.533333        0.333333   \n",
      "\n",
      "       occupation  relationship  race  sex  capital-gain  capital-loss  \\\n",
      "1276     0.857143           0.0   1.0  1.0           0.0      0.453857   \n",
      "35505    0.071429           0.0   1.0  1.0           0.0      0.433196   \n",
      "46820    0.285714           0.0   1.0  1.0           0.0      0.000000   \n",
      "21063    0.214286           0.0   1.0  1.0           0.0      0.000000   \n",
      "31256    0.357143           0.0   1.0  1.0           0.0      0.433196   \n",
      "\n",
      "       hours-per-week  \n",
      "1276         0.397959  \n",
      "35505        0.397959  \n",
      "46820        0.704082  \n",
      "21063        0.448980  \n",
      "31256        0.602041  \n",
      "False Positives Group 1 (first 5 rows):\n",
      "            age  workclass  education  education-num  marital-status  \\\n",
      "32507  0.452055      0.500   0.600000            0.8        0.333333   \n",
      "9086   0.342466      0.500   1.000000            0.6        0.000000   \n",
      "24735  0.589041      0.000   0.600000            0.8        0.333333   \n",
      "31643  0.630137      0.875   1.000000            0.6        0.333333   \n",
      "43754  0.260274      0.875   0.666667            1.0        0.333333   \n",
      "\n",
      "       occupation  relationship  race  sex  capital-gain  capital-loss  \\\n",
      "32507    0.714286           0.0  1.00  1.0           0.0      0.000000   \n",
      "9086     0.500000           0.2  1.00  1.0           0.0      0.365014   \n",
      "24735    0.000000           0.0  0.25  1.0           0.0      0.496556   \n",
      "31643    0.071429           0.0  1.00  1.0           0.0      0.399449   \n",
      "43754    0.714286           0.0  0.25  1.0           0.0      0.000000   \n",
      "\n",
      "       hours-per-week  \n",
      "32507        0.397959  \n",
      "9086         0.397959  \n",
      "24735        0.244898  \n",
      "31643        0.397959  \n",
      "43754        0.397959  \n"
     ]
    }
   ],
   "source": [
    "data, features, target, numeric_columns, categorical_columns = load_adult()\n",
    "\n",
    "target = 'target'\n",
    "attribute = 'sex'\n",
    "classifier_type = 'logistic_regression'\n",
    "\n",
    "# Calculate metrics and get the test DataFrame with predictions\n",
    "results, test_df, trained_model_pipeline = calculate_metrics_and_split_data(data, features, target, classifier_type, attribute=attribute)\n",
    "\n",
    "# Generate DataFrames for true positives and false positives\n",
    "tn_df_0, fn_df_0, tn_df_1, fn_df_1 = generate_tn_fn_dataframes(test_df, target, attribute)\n",
    "\n",
    "print(\"Results:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "print(\"True Positives Group 0 (first 5 rows):\")\n",
    "print(tn_df_0.head())\n",
    "\n",
    "print(\"False Positives Group 0 (first 5 rows):\")\n",
    "print(fn_df_0.head())\n",
    "\n",
    "print(\"True Positives Group 1 (first 5 rows):\")\n",
    "print(tn_df_1.head())\n",
    "\n",
    "print(\"False Positives Group 1 (first 5 rows):\")\n",
    "print(fn_df_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cdae810",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "23",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 23",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m explanations_df\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Apply SP-LIME to each group\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m tn0_explanations_df \u001b[38;5;241m=\u001b[39m apply_splime_to_group(tn_df_0, target, trained_model_pipeline, features)\n\u001b[1;32m     45\u001b[0m fn0_explanations_df \u001b[38;5;241m=\u001b[39m apply_splime_to_group(fn_df_0, target, trained_model_pipeline, features)\n\u001b[1;32m     46\u001b[0m tn1_explanations_df \u001b[38;5;241m=\u001b[39m apply_splime_to_group(tn_df_1, target, trained_model_pipeline, features)\n",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m, in \u001b[0;36mapply_splime_to_group\u001b[0;34m(data_df, target_col, model_pipeline, feature_names, sample_size, num_features, num_exps_desired)\u001b[0m\n\u001b[1;32m     21\u001b[0m explainer \u001b[38;5;241m=\u001b[39m lime_tabular\u001b[38;5;241m.\u001b[39mLimeTabularExplainer(\n\u001b[1;32m     22\u001b[0m     training_data\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m     23\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[1;32m     24\u001b[0m     class_names\u001b[38;5;241m=\u001b[39m[target_col],\n\u001b[1;32m     25\u001b[0m     discretize_continuous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Apply Submodular Pick\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m sp_obj \u001b[38;5;241m=\u001b[39m submodular_pick\u001b[38;5;241m.\u001b[39mSubmodularPick(\n\u001b[1;32m     30\u001b[0m     explainer\u001b[38;5;241m=\u001b[39mexplainer,\n\u001b[1;32m     31\u001b[0m     data\u001b[38;5;241m=\u001b[39mX,  \u001b[38;5;66;03m# Pass the DataFrame directly\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     predict_fn\u001b[38;5;241m=\u001b[39mmodel_pipeline\u001b[38;5;241m.\u001b[39mpredict_proba,\n\u001b[1;32m     33\u001b[0m     sample_size\u001b[38;5;241m=\u001b[39msample_size,\n\u001b[1;32m     34\u001b[0m     num_features\u001b[38;5;241m=\u001b[39mnum_features,\n\u001b[1;32m     35\u001b[0m     num_exps_desired\u001b[38;5;241m=\u001b[39mnum_exps_desired\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Create a DataFrame of the selected explanations\u001b[39;00m\n\u001b[1;32m     39\u001b[0m explanations_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\u001b[38;5;28mdict\u001b[39m(exp\u001b[38;5;241m.\u001b[39mas_list()) \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m sp_obj\u001b[38;5;241m.\u001b[39mexplanations])\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/lime/submodular_pick.py:73\u001b[0m, in \u001b[0;36mSubmodularPick.__init__\u001b[0;34m(self, explainer, data, predict_fn, method, sample_size, num_exps_desired, num_features, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplanations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m sample_indices:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplanations\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     72\u001b[0m         explainer\u001b[38;5;241m.\u001b[39mexplain_instance(\n\u001b[0;32m---> 73\u001b[0m             data[i], predict_fn, num_features\u001b[38;5;241m=\u001b[39mnum_features,\n\u001b[1;32m     74\u001b[0m             top_labels\u001b[38;5;241m=\u001b[39mtop_labels,\n\u001b[1;32m     75\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Error handling\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 23"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from lime import lime_tabular, submodular_pick\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def apply_splime_to_group(data_df, target_col, model_pipeline, feature_names, sample_size=20, num_features=14, num_exps_desired=5):\n",
    "    \"\"\"\n",
    "    Apply SP-LIME to a specific group of instances.\n",
    "\n",
    "    :param data_df: DataFrame for the group (e.g., TN0, FN0)\n",
    "    :param target_col: Name of the target column\n",
    "    :param model_pipeline: The trained model pipeline\n",
    "    :param feature_names: List of feature column names\n",
    "    :param sample_size: Number of samples to draw for submodular pick\n",
    "    :param num_features: Number of features to include in each explanation\n",
    "    :param num_exps_desired: Number of explanations to select in submodular pick\n",
    "    :return: DataFrame of selected explanations\n",
    "    \"\"\"\n",
    "    # Prepare data for LIME\n",
    "    X = data_df[feature_names].reset_index(drop=True)  # Reset index to avoid KeyError\n",
    "    explainer = lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X.values,\n",
    "        feature_names=feature_names,\n",
    "        class_names=[target_col],\n",
    "        discretize_continuous=True\n",
    "    )\n",
    "\n",
    "    # Apply Submodular Pick\n",
    "    sp_obj = submodular_pick.SubmodularPick(\n",
    "        explainer=explainer,\n",
    "        data=X,  # Pass the DataFrame directly\n",
    "        predict_fn=model_pipeline.predict_proba,\n",
    "        sample_size=sample_size,\n",
    "        num_features=num_features,\n",
    "        num_exps_desired=num_exps_desired\n",
    "    )\n",
    "\n",
    "    # Create a DataFrame of the selected explanations\n",
    "    explanations_df = pd.DataFrame([dict(exp.as_list()) for exp in sp_obj.explanations])\n",
    "\n",
    "    return explanations_df\n",
    "\n",
    "# Apply SP-LIME to each group\n",
    "tn0_explanations_df = apply_splime_to_group(tn_df_0, target, trained_model_pipeline, features)\n",
    "fn0_explanations_df = apply_splime_to_group(fn_df_0, target, trained_model_pipeline, features)\n",
    "tn1_explanations_df = apply_splime_to_group(tn_df_1, target, trained_model_pipeline, features)\n",
    "fn1_explanations_df = apply_splime_to_group(fn_df_1, target, trained_model_pipeline, features)\n",
    "\n",
    "# Print the explanation DataFrames for each group\n",
    "print(\"TN0 Group Explanations:\")\n",
    "print(tn0_explanations_df.head())\n",
    "\n",
    "print(\"\\nFN0 Group Explanations:\")\n",
    "print(fn0_explanations_df.head())\n",
    "\n",
    "print(\"\\nTN1 Group Explanations:\")\n",
    "print(tn1_explanations_df.head())\n",
    "\n",
    "print(\"\\nFN1 Group Explanations:\")\n",
    "print(fn1_explanations_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc933b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
